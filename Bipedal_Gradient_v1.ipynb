{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='color:red'> Work in progress - Unfortunately this code is not running at the moment </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook presents the early results working on the bipedal walker environment with a deterministic policy gradient approach: actor-critic model\n",
    "\n",
    "The code to implement the algorithm is inspired from:  \n",
    "\n",
    "https://github.com/artem-oppermann/Deep-Reinforcement-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YrLBVYxi_DKE"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import gym\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import imageio\n",
    "import IPython\n",
    "import sys\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the classes for the actor-critic algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This class implements the OpenAI Gym Bipedal Walker v3\n",
    "        \"\"\"\n",
    "        self.env = gym.make('BipedalWalker-v3')\n",
    "        self.state_size = len(self.env.observation_space.sample())\n",
    "        self.action_size = len(self.env.action_space.sample())\n",
    "        self.images = []\n",
    "        \n",
    "    def get_env(self):\n",
    "        '''Getter function for the OpenAI Gym instance '''\n",
    "        return self.env\n",
    "    \n",
    "    def reset(self):\n",
    "        self.images = []\n",
    "        initial_state = self.env.reset()\n",
    "        return initial_state\n",
    "    \n",
    "    def get_state_size(self):\n",
    "        '''Getter function for the state-size in the environment '''\n",
    "        return self.state_size\n",
    "\n",
    "    def get_action_size(self):\n",
    "        '''Getter function for the state-size in the environment '''\n",
    "        return self.action_size\n",
    "    \n",
    "    def render(self):\n",
    "        '''Adds the every image of the rendering to a list'''\n",
    "        img = self.env.render(mode='rgb_array')\n",
    "        self.images.append(img)\n",
    "\n",
    "    def make_gif(self, filename=\"render\"):\n",
    "        imageio.mimsave(filename + '.gif', [np.array(img) for i, img in enumerate(self.images) if i%2 == 0], fps=29)\n",
    "        return Image(open(filename + '.gif','rb').read())\n",
    "    \n",
    "    def run_random_episode(self):\n",
    "        current_state = self.reset()\n",
    "        final_state = False\n",
    "        iters = 0\n",
    "        \n",
    "        while not final_state and iters < 1000:\n",
    "            self.render()\n",
    "            action = self.env.action_space.sample()\n",
    "            current_state, reward, final_state, info = self.env.step(action)\n",
    "            iters += 1\n",
    "        env.close()\n",
    "\n",
    "        return self.make_gif(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    \n",
    "    def __init__(self, scope, target_network, env, flags):\n",
    "        \n",
    "        \"\"\"\n",
    "        This class implements the actor for the deterministic policy gradients model.\n",
    "        The actor class determines the action that the agent must take in a environment.\n",
    "    \n",
    "        :param scope: within this scope the parameters will be defined\n",
    "        :param target_network: instance of the Actor(target-network class)\n",
    "        :param env: instance of the openAI environment\n",
    "        :param FLAGS: TensorFlow flags which contain thevalues for hyperparameters\n",
    "    \n",
    "        \"\"\"\n",
    "        \n",
    "        self.TF_FLAGS = flags\n",
    "        self.env = env\n",
    "        \n",
    "        if scope=='target':\n",
    "            \n",
    "            with tf.variable_scope(scope):\n",
    "                \n",
    "                self.state = tf.placeholder(tf.float32, shape=(None, self.env.get_state_size()), name='state')\n",
    "                self.action = self.action_estimator()\n",
    "                self.param = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope)\n",
    "\n",
    "        elif scope=='actor':\n",
    "            \n",
    "            with tf.variable_scope(scope):\n",
    "                \n",
    "                self.state = tf.placeholder(tf.float32, shape=(None, self.env.get_state_size()), name='state')\n",
    "                self.target_network = target_network\n",
    "\n",
    "                self.q_network_gradient = tf.placeholder(tf.float32, shape=(None,self.env.get_action_size()), name='q_network_gradients')\n",
    "                self.action=self.action_estimator()\n",
    "                \n",
    "                self.param = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope)\n",
    "                \n",
    "                with tf.name_scope('policy_gradients'):\n",
    "                    self.unnormalized_gradients = tf.gradients(self.action, self.param, -self.q_network_gradient)\n",
    "                    self.policy_gradient=list(map(lambda x: tf.div(x, 1), self.unnormalized_gradients))\n",
    "                    \n",
    "                with tf.name_scope('train_policy_network'):\n",
    "                    self.train_opt=tf.train.AdamOptimizer(self.TF_FLAGS.learning_rate_Actor).apply_gradients(zip(self.policy_gradient,self.param))\n",
    "                \n",
    "                with tf.name_scope('update_actor_target'):     \n",
    "                    self.update_opt=[tp.assign(tf.multiply(self.TF_FLAGS.tau,lp)+tf.multiply(1-self.TF_FLAGS.tau,tp)) for tp, lp in zip(self.target_network.param,self.param)]\n",
    "                          \n",
    "                with tf.name_scope('initialize_actor_target_network'):\n",
    "                     self.init_target_op=[tp.assign(lp) for tp, lp in zip(self.target_network.param,self.param)]\n",
    "                    \n",
    "    def action_estimator(self):\n",
    "        '''Build the neural network that estimates the action for a given state '''\n",
    "        \n",
    "        h1 = tf.layers.dense(\n",
    "                             self.state, 8, tf.nn.relu, use_bias=None,\n",
    "                             kernel_initializer=tf.random_normal_initializer(),\n",
    "                             bias_initializer=tf.zeros_initializer()\n",
    "                             )\n",
    "\n",
    "        actions = tf.layers.dense(h1, self.env.get_action_size(), None, kernel_initializer=tf.random_normal_initializer())  \n",
    "        \n",
    "        min_action = self.env.get_env().action_space.low\n",
    "        max_action = self.env.get_env().action_space.high\n",
    "\n",
    "        scalled_actions = min_action + tf.nn.sigmoid(actions)*(max_action - min_action)\n",
    "        \n",
    "        return scalled_actions\n",
    "    \n",
    "    def set_session(self, session):\n",
    "        '''Set the session '''\n",
    "        self.session=session\n",
    "    \n",
    "    def init_target_network(self):\n",
    "        '''Initialize the parameters of the target-network '''\n",
    "        self.session.run(self.init_target_op)\n",
    "\n",
    "    def update_target_parameter(self):\n",
    "        '''Update the parameters of the target-network '''\n",
    "        self.session.run(self.update_opt)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        '''Get an action for a certain state '''\n",
    "        return self.session.run(self.action, feed_dict={self.state: state})\n",
    "\n",
    "    def train(self, state, q_gradient):\n",
    "        '''Train the actor network '''\n",
    "        feed_dict={\n",
    "                    self.q_network_gradient: q_gradient,\n",
    "                    self.state: state }\n",
    "        self.session.run(self.train_opt,feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    \n",
    "    def __init__(self, scope, target_network, env, flags):\n",
    "        \"\"\"\n",
    "        This class implements the Critic for the stochastic policy gradient model.\n",
    "        The critic provides a state-value for the current state environment where \n",
    "        the agent operates.\n",
    "        \n",
    "        :param scope: within this scope the parameters will be defined\n",
    "        :param target_network: instance of the Actor(target-network class)\n",
    "        :param env: instance of the openAI environment\n",
    "        :param FLAGS: TensorFlow flags which contain thevalues for hyperparameters\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.TF_FLAGS = flags\n",
    "        self.env = env\n",
    "        \n",
    "        if scope=='target':\n",
    "            \n",
    "            with tf.variable_scope(scope):\n",
    "                \n",
    "                self.gamma = 0.99\n",
    "                self.state = tf.placeholder(tf.float32, shape=(None,self.env.state_size), name='state')\n",
    "                self.actions = tf.placeholder(tf.float32, shape=(None,self.env.get_action_size()), name='actions')\n",
    "                self.q = self.action_value_estimator(scope='q_target_network')\n",
    "                self.param = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/q_target_network')\n",
    "\n",
    "        else:\n",
    "            \n",
    "            with tf.variable_scope(scope):\n",
    "                \n",
    "                self.target_network=target_network\n",
    "\n",
    "                self.state = tf.placeholder(tf.float32, shape=(None,self.env.get_state_size()), name='state')\n",
    "                self.target = tf.placeholder(tf.float32, shape=(None,self.env.get_action_size()), name='target')\n",
    "                self.actions = tf.placeholder(tf.float32, shape=(None,self.env.get_action_size()), name='actions')\n",
    "                \n",
    "                self.q = self.action_value_estimator(scope='q_network')\n",
    "\n",
    "                self.param = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/q_network')\n",
    "                \n",
    "                with tf.name_scope('q_network_loss'):\n",
    "                    loss=tf.losses.mean_squared_error(self.target, self.q)\n",
    "            \n",
    "                with tf.name_scope('q_network_gradient'):\n",
    "                    self.gradients=tf.gradients(self.q, self.actions)\n",
    "            \n",
    "                with tf.name_scope('train_q_network'):\n",
    "                    self.train_opt=tf.train.AdamOptimizer(self.TF_FLAGS.learning_rate_Critic).minimize(loss)\n",
    "            \n",
    "                with tf.name_scope('update_q_target'):    \n",
    "                    self.update_opt=[tp.assign(tf.multiply(self.TF_FLAGS.tau,lp)+tf.multiply(1-self.TF_FLAGS.tau,tp)) for tp, lp in zip(self.target_network.param,self.param)]\n",
    "                    \n",
    "                with tf.name_scope('initialize_q_target_network'):\n",
    "                     self.init_target_op=[tp.assign(lp) for tp, lp in zip(self.target_network.param,self.param)]\n",
    " \n",
    "    def action_value_estimator(self, scope):    \n",
    "        '''Build the neural network that estimates the action-values '''\n",
    "        \n",
    "        state_action = tf.concat([self.state, self.actions], axis=1)\n",
    "        \n",
    "        with tf.variable_scope(scope):\n",
    "             \n",
    "            h1 = tf.layers.dense(state_action, 8, tf.nn.relu,use_bias=None,\n",
    "                                 kernel_initializer=tf.random_normal_initializer(),\n",
    "                                 bias_initializer=tf.zeros_initializer()\n",
    "                                 )\n",
    "            \n",
    "            q = tf.layers.dense(h1, self.env.get_action_size(), None,\n",
    "                                kernel_initializer=tf.random_normal_initializer())                     \n",
    "        return q\n",
    "    \n",
    "    \n",
    "    def compute_gradients(self, state, actions):\n",
    "        '''Compute the gradients of the action_value estimator neural network '''\n",
    "        \n",
    "        feed_dict={self.state: state, \n",
    "                   self.actions:actions\n",
    "                   }\n",
    "        \n",
    "        q_gradient=self.session.run(self.gradients, feed_dict)\n",
    "        q_gradient=np.array(q_gradient).reshape(1, -1)\n",
    "        \n",
    "        return q_gradient\n",
    "    \n",
    "    \n",
    "    def calculate_Q(self, state, actions):\n",
    "        '''Compute the action-value '''\n",
    "\n",
    "        feed_dict={self.state: state,\n",
    "                   self.actions:actions}\n",
    "        \n",
    "        q_next=self.session.run(self.q,feed_dict)\n",
    "        \n",
    "        return q_next\n",
    "    \n",
    "    \n",
    "    def train(self, state, targets, action):\n",
    "        '''Train the actor network '''\n",
    "        \n",
    "        feed_dict={\n",
    "                    self.state: state, \n",
    "                    self.target: targets, \n",
    "                    self.actions: action\n",
    "                   }\n",
    "        self.session.run(self.train_opt,feed_dict)\n",
    "    \n",
    "    \n",
    "    def set_session(self, session):\n",
    "        '''Set the session '''\n",
    "        self.session=session\n",
    "    \n",
    "    \n",
    "    def init_target_network(self):\n",
    "        '''Initialize the parameters of the target-network '''\n",
    "        self.session.run(self.init_target_op)\n",
    "             \n",
    "       \n",
    "    def update_target_parameter(self):\n",
    "        '''Update the parameters of the target-network '''\n",
    "        self.session.run(self.update_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, TF_FLAGS):\n",
    "        ''' This class build the Agent that learns in the environment via the actor-critic algorithm. '''\n",
    "        self.env = Environment()\n",
    "        self.TF_FLAGS = TF_FLAGS\n",
    "                \n",
    "        self.actor_target = Actor(scope='target',target_network=None,env=self.env, flags=TF_FLAGS)\n",
    "        self.actor = Actor(scope='actor',target_network=self.actor_target,env=self.env, flags=TF_FLAGS)\n",
    "        \n",
    "        self.critic_target = Critic(scope='target',target_network=None,env=self.env, flags=TF_FLAGS)\n",
    "        self.critic = Critic(scope='critic',target_network=self.critic_target,env=self.env, flags=TF_FLAGS)\n",
    "        \n",
    "        self.session = tf.InteractiveSession()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        \n",
    "        self.critic.set_session(self.session)\n",
    "        self.actor.set_session(self.session)\n",
    "        self.actor_target.set_session(self.session)\n",
    "        self.critic_target.set_session(self.session)\n",
    "        \n",
    "        self.critic.init_target_network()\n",
    "        self.actor.init_target_network()\n",
    "                \n",
    "    def update_critic(self, state, action, reward, next_state, done):\n",
    "        next_action = self.actor_target.get_action(next_state)\n",
    "        q_next = self.critic.target_network.calculate_Q(next_state, next_action)\n",
    "#       target = np.array([r+self.FLAGS.gamma*q if not done else r for r, q, done in zip(reward, q_next, done)])\n",
    "        target = np.array(reward+self.TF_FLAGS.gamma*q_next)\n",
    "        self.critic.train(state, target, action)\n",
    "        self.critic.update_target_parameter()\n",
    "    \n",
    "    def update_actor(self, state, action, reward, next_state, done):\n",
    "        current_action = self.actor.get_action(state)        \n",
    "        q_gradient = self.critic.compute_gradients(state, current_action)\n",
    "        self.actor.train(state, q_gradient)\n",
    "        self.actor.update_target_parameter()\n",
    "        \n",
    "    def train_one_episode(self):\n",
    "        ''' Play an episode in the OpenAI Gym '''\n",
    "        # Get the initial state and reshape it\n",
    "        state = self.env.reset()\n",
    "        state = state.reshape(1,self.env.get_state_size())\n",
    "        \n",
    "        done=False\n",
    "        total_reward=0\n",
    "        iters = 0\n",
    "        \n",
    "        # Loop for the episode\n",
    "        while not done and iters < 2000:\n",
    "            \n",
    "            # Sample an action from the actor distribution\n",
    "            action = self.actor.get_action(state)\n",
    "            prev_state = state\n",
    "            \n",
    "            # Obtain a <state, reward, done> tuple from the environment\n",
    "            state, reward, done, _ = self.env.get_env().step(action.reshape(-1))\n",
    "            total_reward = total_reward + reward\n",
    "            \n",
    "            state = state.reshape(1, self.env.get_state_size())\n",
    "            prev_state = prev_state.reshape(1, self.env.get_state_size())\n",
    "            action = action.reshape(1, self.env.get_action_size())\n",
    "            \n",
    "            self.update_critic(prev_state, action, reward, state, done)\n",
    "            self.update_actor(prev_state, action, reward, state, done)\n",
    "            \n",
    "            iters += 1\n",
    "            \n",
    "        return total_reward\n",
    "            \n",
    "    def train(self, num_episodes=100):\n",
    "        '''Run the environment for a particular number of episodes. '''\n",
    "        total_rewards = np.empty(num_episodes+1)\n",
    "        n_steps=1\n",
    "        \n",
    "        for n in range(0, num_episodes+1):\n",
    "\n",
    "            total_reward = self.train_one_episode()\n",
    "            total_rewards[n] = total_reward \n",
    "            \n",
    "            if n%10==0:\n",
    "                print(\"episodes: %i, avg_reward (last: %i episodes): %.2f\" %(n, n_steps, total_rewards[max(0, n-n_steps):(n+1)].mean()))\n",
    "                \n",
    "    def record_episode(self):\n",
    "        '''Runs and records one episode using the trained actor and critic'''\n",
    "        # Get the initial state and reshape it\n",
    "        state=self.env.reset()\n",
    "        state=state.reshape(1,self.env.get_state_size())\n",
    "        done=False\n",
    "        iters = 0\n",
    "        total_reward = 0\n",
    "\n",
    "        # Loop for the episode\n",
    "        while not done and iters < 2000:\n",
    "            \n",
    "            # Sample an action from the gauss distribution\n",
    "            action = self.actor.get_action(state)\n",
    "\n",
    "            # Obtain a <state, reward, done> tuple from the environment\n",
    "            state, reward, done, _ = self.env.get_env().step(action.reshape(-1))\n",
    "            state = state.reshape(1, self.env.get_state_size())\n",
    "            total_reward += reward\n",
    "            \n",
    "            self.env.render()\n",
    "            iters += 1\n",
    "        \n",
    "        return self.env.make_gif()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/utilisateur/opt/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/Users/utilisateur/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodes: 0, avg_reward (last: 1 episodes): -100.34\n",
      "episodes: 10, avg_reward (last: 1 episodes): -150.46\n",
      "episodes: 20, avg_reward (last: 1 episodes): -115.61\n",
      "episodes: 30, avg_reward (last: 1 episodes): -106.33\n",
      "episodes: 40, avg_reward (last: 1 episodes): -101.26\n",
      "episodes: 50, avg_reward (last: 1 episodes): -101.91\n",
      "episodes: 60, avg_reward (last: 1 episodes): -123.29\n",
      "episodes: 70, avg_reward (last: 1 episodes): -124.36\n",
      "episodes: 80, avg_reward (last: 1 episodes): -116.51\n",
      "episodes: 90, avg_reward (last: 1 episodes): -143.95\n",
      "episodes: 100, avg_reward (last: 1 episodes): -102.04\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters\n",
    "sys.argv = sys.argv[:1]\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()\n",
    "    keys_list = [keys for keys in flags_dict]\n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "        \n",
    "try:\n",
    "    del_all_flags(TF_FLAGS)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "tf.app.flags.DEFINE_float('learning_rate_Actor', 0.001, 'Learning rate for the policy estimator')\n",
    "tf.app.flags.DEFINE_float('learning_rate_Critic', 0.001, 'Learning rate for the state-value estimator')\n",
    "tf.app.flags.DEFINE_float('gamma', 0.99, 'Future discount factor')\n",
    "tf.app.flags.DEFINE_float('tau', 1e-2, 'Update rate for the target networks parameter')\n",
    "\n",
    "TF_FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "agent = Agent(TF_FLAGS)\n",
    "\n",
    "agent.train(num_episodes = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.record_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Bipedal_Evolution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
